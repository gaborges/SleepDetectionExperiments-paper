myFlowerExperiment | INFO flwr 2023-09-28 22:27:23,778 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-28 22:27:23,822 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-28 22:27:23,828 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-28 22:27:23,831 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,187 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,187 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,316 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,316 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,319 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,319 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,322 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-28 23:35:02,322 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:12:58,010 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,750 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 16.0, 'node:172.30.126.159': 1.0, 'object_store_memory': 1701682790.0, 'memory': 3403365582.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,751 | app.py:218 | No `client_resources` specified. Using minimal resources for clients.
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,752 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,771 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,774 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:13:01,775 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | ERROR flwr 2023-09-29 00:13:14,680 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 00:13:14,706 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:13:14,707 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=3150, ip=172.30.126.159, actor_id=7fd9b77046f5651af5aff58701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fb9208c4550>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/4059848170.py", line 9, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:13:14,708 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 0.0} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 0.0}.
myFlowerExperiment | INFO flwr 2023-09-29 00:16:14,079 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:16:14,151 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:16:14,154 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:16:14,155 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:18:01,988 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:18:02,045 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:18:02,047 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:18:02,048 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,580 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,580 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,615 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,615 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,618 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,618 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,619 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:22:49,619 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,679 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,679 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,683 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,683 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,686 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,686 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,688 | connection.py:42 | ChannelConnectivity.READY
myFlowerExperiment | DEBUG flwr 2023-09-29 00:22:52,688 | connection.py:42 | ChannelConnectivity.READY
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,711 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,711 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,713 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,713 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,714 | server.py:104 | FL starting
myFlowerExperiment | INFO flwr 2023-09-29 00:22:52,714 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 00:25:50,191 | connection.py:139 | gRPC channel closed
myFlowerExperiment | DEBUG flwr 2023-09-29 00:25:50,191 | connection.py:139 | gRPC channel closed
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,592 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,592 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,638 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,638 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,640 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,640 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,642 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:26:04,642 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,398 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,398 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,411 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,411 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,413 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,413 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,414 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:26:58,414 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:27:01,520 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | INFO flwr 2023-09-29 00:27:01,520 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:01,545 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:01,545 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:01,550 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:01,550 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:21,546 | connection.py:42 | ChannelConnectivity.TRANSIENT_FAILURE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:21,546 | connection.py:42 | ChannelConnectivity.TRANSIENT_FAILURE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:21,748 | connection.py:139 | gRPC channel closed
myFlowerExperiment | DEBUG flwr 2023-09-29 00:27:21,748 | connection.py:139 | gRPC channel closed
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,498 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,498 | app.py:162 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,513 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,513 | app.py:175 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,516 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,516 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,517 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:33:48,517 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:33:51,616 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | INFO flwr 2023-09-29 00:33:51,616 | grpc.py:49 | Opened insecure gRPC connection (no certificates were passed)
myFlowerExperiment | DEBUG flwr 2023-09-29 00:33:51,640 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:33:51,640 | connection.py:42 | ChannelConnectivity.IDLE
myFlowerExperiment | DEBUG flwr 2023-09-29 00:33:51,645 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:33:51,645 | connection.py:42 | ChannelConnectivity.CONNECTING
myFlowerExperiment | DEBUG flwr 2023-09-29 00:34:03,304 | connection.py:139 | gRPC channel closed
myFlowerExperiment | DEBUG flwr 2023-09-29 00:34:03,304 | connection.py:139 | gRPC channel closed
myFlowerExperiment | INFO flwr 2023-09-29 00:34:49,102 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:34:49,102 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,666 | app.py:210 | Flower VCE: Ray initialized with resources: {'memory': 2905600820.0, 'object_store_memory': 1452800409.0, 'CPU': 16.0, 'node:__internal_head__': 1.0, 'node:172.30.126.159': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,666 | app.py:210 | Flower VCE: Ray initialized with resources: {'memory': 2905600820.0, 'object_store_memory': 1452800409.0, 'CPU': 16.0, 'node:__internal_head__': 1.0, 'node:172.30.126.159': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,668 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8}
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,668 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8}
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,694 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,694 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,696 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,696 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,697 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:34:55,697 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,973 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,973 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,974 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,974 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,975 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,975 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=5147, ip=172.30.126.159, actor_id=09e242941190bc397ff2475d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f72500ff4c0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/195030271.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,976 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8}.
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:02,976 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8}.
myFlowerExperiment | INFO flwr 2023-09-29 00:35:22,670 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:35:22,670 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,411 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 16.0, 'object_store_memory': 1411360358.0, 'memory': 2822720718.0, 'node:172.30.126.159': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,411 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 16.0, 'object_store_memory': 1411360358.0, 'memory': 2822720718.0, 'node:172.30.126.159': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,413 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,413 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,427 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,427 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,429 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,429 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,431 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 00:35:27,431 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,713 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,713 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,725 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,725 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,727 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,727 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=6013, ip=172.30.126.159, actor_id=9c67370032fd58d316166c8201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9b2ca85430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_2517/3144413284.py", line 11, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,728 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1}.
myFlowerExperiment | ERROR flwr 2023-09-29 00:35:44,728 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1}.
myFlowerExperiment | INFO flwr 2023-09-29 15:58:40,932 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 15:58:47,177 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'node:__internal_head__': 1.0, 'CPU': 16.0, 'object_store_memory': 1883335065.0, 'memory': 3766670132.0}
myFlowerExperiment | INFO flwr 2023-09-29 15:58:47,248 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 15:58:47,992 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 15:58:48,003 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 15:58:48,007 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | ERROR flwr 2023-09-29 15:59:09,053 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 15:59:09,085 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 15:59:09,089 | app.py:294 | [36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters
    client: Client = _create_client(self.client_fn, self.cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client
    client_like: ClientLike = client_fn(cid)
  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn
TypeError: list indices must be integers or slices, not str

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1367, ip=172.30.126.159, actor_id=e1f5c66224e063dbf663ad8d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8bb5384490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 178, in get_parameters\n    client: Client = _create_client(self.client_fn, self.cid)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 284, in _create_client\n    client_like: ClientLike = client_fn(cid)\n  File "/tmp/ipykernel_695/1833633559.py", line 14, in client_fn\nTypeError: list indices must be integers or slices, not str\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 15:59:09,091 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1}.
myFlowerExperiment | INFO flwr 2023-09-29 16:10:09,142 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:10:17,143 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'CPU': 16.0, 'node:__internal_head__': 1.0, 'object_store_memory': 1772820480.0, 'memory': 3545640960.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:10:17,146 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:10:17,269 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:10:17,273 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:10:17,275 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:10:32,202 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:10:32,208 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:10:32,211 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:10:32,217 | server.py:222 | fit_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:10:53,620 | server.py:236 | fit_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:10:53,666 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:10:53,669 | server.py:173 | evaluate_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:13,259 | server.py:187 | evaluate_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:11:13,262 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:13,264 | server.py:222 | fit_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:14,239 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:14,242 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:28,217 | server.py:236 | fit_round 2 received 2 results and 1 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:28,225 | server.py:173 | evaluate_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:28,840 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:28,842 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:40,775 | server.py:187 | evaluate_round 2 received 2 results and 1 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:40,777 | server.py:222 | fit_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:41,449 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:41,450 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:41,634 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 74f40f4d55eee988f37dea5901000000, name=DefaultActor.__init__, pid=2450, memory used=0.61GB) was running was 7.11GB / 7.44GB (0.955402), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.59	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2450	0.61	ray::DefaultActor.run
2448	0.60	ray::DefaultActor.run
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
2442	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:41,636 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 74f40f4d55eee988f37dea5901000000, name=DefaultActor.__init__, pid=2450, memory used=0.61GB) was running was 7.11GB / 7.44GB (0.955402), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.59	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2450	0.61	ray::DefaultActor.run
2448	0.60	ray::DefaultActor.run
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
2442	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:53,222 | server.py:236 | fit_round 3 received 1 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:11:53,226 | server.py:173 | evaluate_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:53,817 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 74f40f4d55eee988f37dea5901000000, name=DefaultActor.__init__, pid=2450, memory used=0.61GB) was running was 7.11GB / 7.44GB (0.955402), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.59	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2450	0.61	ray::DefaultActor.run
2448	0.60	ray::DefaultActor.run
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
2442	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:53,819 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:11:53,819 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 74f40f4d55eee988f37dea5901000000, name=DefaultActor.__init__, pid=2450, memory used=0.61GB) was running was 7.11GB / 7.44GB (0.955402), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-63486985113fb394e66e288aa2c1d03f0a5abfd6d519518ac5ccd904*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.59	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2450	0.61	ray::DefaultActor.run
2448	0.60	ray::DefaultActor.run
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
2442	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2023-09-29 16:11:53,821 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: bc9b61d4a26659fe49e6eef6296f63c2a8aa37fd859759ef0a47c7fd) where the task (actor ID: 868b527018f4cf28b54c221101000000, name=DefaultActor.__init__, pid=2449, memory used=0.48GB) was running was 7.07GB / 7.44GB (0.950507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5a11f691f2ae98aa6c952b5374fb38b64f75ef1cbd6f9f81fec6ca18*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	1.41	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
2449	0.48	ray::DefaultActor.run
2450	0.46	ray::DefaultActor
2448	0.46	ray::DefaultActor
2441	0.25	ray::DefaultActor
2439	0.25	ray::DefaultActor
2446	0.25	ray::DefaultActor
2443	0.25	ray::DefaultActor
2447	0.25	ray::DefaultActor
2435	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2023-09-29 16:12:05,406 | server.py:187 | evaluate_round 3 received 1 results and 2 failures
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,408 | server.py:153 | FL finished in 93.1941661430028
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,462 | app.py:225 | app_fit: losses_distributed [(1, 0.4834057688713074), (2, 0.4909222722053528), (3, 0.5341633558273315)]
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,466 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,467 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,469 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 16:12:05,471 | app.py:229 | app_fit: metrics_centralized {}
myFlowerExperiment | INFO flwr 2023-09-29 16:12:50,033 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:12:58,015 | app.py:210 | Flower VCE: Ray initialized with resources: {'memory': 3730236212.0, 'object_store_memory': 1865118105.0, 'node:172.30.126.159': 1.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:12:58,017 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:12:58,047 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:12:58,049 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:12:58,051 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:13:17,666 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:13:17,668 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:13:17,670 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:13:17,672 | server.py:222 | fit_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:13:37,251 | server.py:236 | fit_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:13:37,259 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:13:37,261 | server.py:173 | evaluate_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:13:52,067 | server.py:187 | evaluate_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:13:52,069 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:13:52,071 | server.py:222 | fit_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:08,942 | server.py:236 | fit_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:08,946 | server.py:173 | evaluate_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:27,980 | server.py:187 | evaluate_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:27,982 | server.py:222 | fit_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:14:29,850 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 15da5b351f19d0049c41b33adc6805f98ff1317d1bcf51c59828a782) where the task (actor ID: 703c84d71dd58ce03bc0d11701000000, name=DefaultActor.__init__, pid=6270, memory used=0.61GB) was running was 7.07GB / 7.44GB (0.950082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	0.93	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
6270	0.61	ray::DefaultActor.run
6268	0.60	ray::DefaultActor.run
6269	0.60	ray::DefaultActor.run
6258	0.25	ray::DefaultActor
6256	0.25	ray::DefaultActor
6262	0.25	ray::DefaultActor
6265	0.25	ray::DefaultActor
6253	0.25	ray::DefaultActor
6259	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:14:29,853 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 15da5b351f19d0049c41b33adc6805f98ff1317d1bcf51c59828a782) where the task (actor ID: 703c84d71dd58ce03bc0d11701000000, name=DefaultActor.__init__, pid=6270, memory used=0.61GB) was running was 7.07GB / 7.44GB (0.950082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	0.93	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
6270	0.61	ray::DefaultActor.run
6268	0.60	ray::DefaultActor.run
6269	0.60	ray::DefaultActor.run
6258	0.25	ray::DefaultActor
6256	0.25	ray::DefaultActor
6262	0.25	ray::DefaultActor
6265	0.25	ray::DefaultActor
6253	0.25	ray::DefaultActor
6259	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:43,364 | server.py:236 | fit_round 3 received 2 results and 1 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:14:43,368 | server.py:173 | evaluate_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | ERROR flwr 2023-09-29 16:14:43,983 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 15da5b351f19d0049c41b33adc6805f98ff1317d1bcf51c59828a782) where the task (actor ID: 703c84d71dd58ce03bc0d11701000000, name=DefaultActor.__init__, pid=6270, memory used=0.61GB) was running was 7.07GB / 7.44GB (0.950082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	0.93	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
6270	0.61	ray::DefaultActor.run
6268	0.60	ray::DefaultActor.run
6269	0.60	ray::DefaultActor.run
6258	0.25	ray::DefaultActor
6256	0.25	ray::DefaultActor
6262	0.25	ray::DefaultActor
6265	0.25	ray::DefaultActor
6253	0.25	ray::DefaultActor
6259	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 16:14:43,985 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 15da5b351f19d0049c41b33adc6805f98ff1317d1bcf51c59828a782) where the task (actor ID: 703c84d71dd58ce03bc0d11701000000, name=DefaultActor.__init__, pid=6270, memory used=0.61GB) was running was 7.07GB / 7.44GB (0.950082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5fcb357a6cd55a6f8d2a01bdbb75bf18ab96c80ffbb5344f3c12a93e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
695	0.93	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
6270	0.61	ray::DefaultActor.run
6268	0.60	ray::DefaultActor.run
6269	0.60	ray::DefaultActor.run
6258	0.25	ray::DefaultActor
6256	0.25	ray::DefaultActor
6262	0.25	ray::DefaultActor
6265	0.25	ray::DefaultActor
6253	0.25	ray::DefaultActor
6259	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | INFO flwr 2023-09-29 16:27:06,978 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:27:18,393 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'node:__internal_head__': 1.0, 'memory': 4168718747.0, 'object_store_memory': 2084359372.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:27:18,395 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:27:18,444 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:27:18,446 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:27:18,448 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:27:39,909 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:27:39,911 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:27:39,912 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:27:39,914 | server.py:222 | fit_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:27:57,454 | server.py:236 | fit_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:27:57,473 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:27:57,475 | server.py:173 | evaluate_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:15,264 | server.py:187 | evaluate_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:28:15,265 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:15,267 | server.py:222 | fit_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:31,071 | server.py:236 | fit_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:31,077 | server.py:173 | evaluate_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:48,738 | server.py:187 | evaluate_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:28:48,739 | server.py:222 | fit_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:29:05,949 | server.py:236 | fit_round 3 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:29:05,954 | server.py:173 | evaluate_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:29:24,440 | server.py:187 | evaluate_round 3 received 3 results and 0 failures
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,442 | server.py:153 | FL finished in 104.52753550099806
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,445 | app.py:225 | app_fit: losses_distributed [(1, 0.4744991958141327), (2, 0.4825557768344879), (3, 0.49046656489372253)]
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,447 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,449 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,450 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 16:29:24,451 | app.py:229 | app_fit: metrics_centralized {}
myFlowerExperiment | INFO flwr 2023-09-29 16:29:34,969 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:29:44,396 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 16.0, 'node:172.30.126.159': 1.0, 'object_store_memory': 2107321958.0, 'memory': 4214643918.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:29:44,398 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:29:44,444 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:29:44,448 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:29:44,452 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:29:56,281 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:29:56,283 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:29:56,284 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:29:56,286 | server.py:222 | fit_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:14,616 | server.py:236 | fit_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:30:14,623 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:14,625 | server.py:173 | evaluate_round 1: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:29,582 | server.py:187 | evaluate_round 1 received 3 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:30:29,584 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:29,585 | server.py:222 | fit_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:44,060 | server.py:236 | fit_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:44,065 | server.py:173 | evaluate_round 2: strategy sampled 3 clients (out of 3)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:58,110 | server.py:187 | evaluate_round 2 received 3 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:30:58,112 | server.py:222 | fit_round 3: strategy sampled 3 clients (out of 3)
myFlowerExperiment | INFO flwr 2023-09-29 16:31:58,007 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:32:04,549 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:172.30.126.159': 1.0, 'memory': 4308212123.0, 'object_store_memory': 2154106060.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:32:04,551 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:32:04,574 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:32:04,577 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:32:04,579 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:32:15,052 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:32:15,053 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:32:15,055 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:15,057 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:27,726 | server.py:236 | fit_round 1 received 2 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:32:27,730 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:27,732 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:38,460 | server.py:187 | evaluate_round 1 received 2 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:32:38,461 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:38,463 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:51,489 | server.py:236 | fit_round 2 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:32:51,493 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:33:05,653 | server.py:187 | evaluate_round 2 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:33:05,655 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:33:22,646 | server.py:236 | fit_round 3 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:33:22,651 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:33:38,924 | server.py:187 | evaluate_round 3 received 2 results and 0 failures
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,926 | server.py:153 | FL finished in 83.86966819900044
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,928 | app.py:225 | app_fit: losses_distributed [(1, 0.4652746319770813), (2, 0.492205411195755), (3, 0.5040726661682129)]
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,929 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,930 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,931 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 16:33:38,933 | app.py:229 | app_fit: metrics_centralized {}
myFlowerExperiment | INFO flwr 2023-09-29 16:36:48,021 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:36:53,111 | app.py:210 | Flower VCE: Ray initialized with resources: {'CPU': 16.0, 'node:__internal_head__': 1.0, 'object_store_memory': 1802408755.0, 'memory': 3604817511.0, 'node:172.30.126.159': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:36:53,113 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:36:53,137 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:36:53,140 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:36:53,143 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:37:05,786 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:37:05,788 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:37:05,790 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:05,792 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:22,698 | server.py:236 | fit_round 1 received 2 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:37:22,706 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:22,708 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:37:38,145 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:37:38,147 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:38,148 | server.py:187 | evaluate_round 1 received 1 results and 1 failures
myFlowerExperiment | WARNING flwr 2023-09-29 16:37:38,150 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:38,152 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:56,281 | server.py:236 | fit_round 2 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:37:56,286 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:38:11,251 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:38:11,253 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:38:11,450 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:38:11,452 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:38:11,453 | server.py:187 | evaluate_round 2 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:38:11,454 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:38:27,858 | server.py:236 | fit_round 3 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:38:27,863 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:38:41,810 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:38:41,812 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21281, ip=172.30.126.159, actor_id=1490ae9ab050df56c88a131701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7fd4228400>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:38:42,019 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:38:42,021 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=21282, ip=172.30.126.159, actor_id=02e0a7d7d4d8058c6406671001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fcac8e17430>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3335566265.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:38:42,024 | server.py:187 | evaluate_round 3 received 0 results and 2 failures
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,026 | server.py:153 | FL finished in 96.23399696900015
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,045 | app.py:225 | app_fit: losses_distributed [(1, 0.45521315932273865)]
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,047 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,048 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,049 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 16:38:42,050 | app.py:229 | app_fit: metrics_centralized {}
myFlowerExperiment | INFO flwr 2023-09-29 16:39:23,423 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 16:39:31,490 | app.py:210 | Flower VCE: Ray initialized with resources: {'CPU': 16.0, 'object_store_memory': 1851937996.0, 'memory': 3703875995.0, 'node:172.30.126.159': 1.0, 'node:__internal_head__': 1.0}
myFlowerExperiment | INFO flwr 2023-09-29 16:39:31,492 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 16:39:31,519 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 16:39:31,522 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:39:31,523 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:39:46,821 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 16:39:46,823 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 16:39:46,825 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 16:39:46,827 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:01,769 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:01,771 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:02,366 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:02,367 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:02,369 | server.py:236 | fit_round 1 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:02,372 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:18,706 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:18,708 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:21,966 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:21,968 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:21,970 | server.py:187 | evaluate_round 1 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:21,973 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:38,022 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:38,024 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:38,711 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:38,713 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:38,715 | server.py:236 | fit_round 2 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:38,716 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:52,023 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:52,025 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:40:52,355 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:40:52,357 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:52,359 | server.py:187 | evaluate_round 2 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:40:52,361 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:41:07,398 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:41:07,400 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:41:07,405 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:41:07,406 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit
    return maybe_call_fit(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 196, in fit\n    return maybe_call_fit(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 217, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 333, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 27, in fit\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:41:07,408 | server.py:236 | fit_round 3 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 16:41:07,410 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 16:41:21,450 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:41:21,452 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24420, ip=172.30.126.159, actor_id=91a830aa5d89bccdc433520801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7914ef2460>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 16:41:21,899 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 16:41:21,901 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns
    raise AssertionError(
AssertionError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run
    job_results = job_fn()
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__
    arrays, columns, index = nested_data_to_arrays(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays
    arrays, columns = to_arrays(data, columns, dtype=dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays
    content, columns = _finalize_columns_and_data(arr, columns, dtype)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data
    raise ValueError(err) from err
ValueError: 15 columns passed, passed data had 16 columns

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24421, ip=172.30.126.159, actor_id=343b9669537ad18a25c6ca8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5fff5c1370>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 969, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 1017, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 15 columns passed, passed data had 16 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/1356611328.py", line 46, in evaluate_and_save_results\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py", line 745, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 510, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 875, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py", line 972, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 15 columns passed, passed data had 16 columns\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 16:41:21,903 | server.py:187 | evaluate_round 3 received 0 results and 2 failures
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,904 | server.py:153 | FL finished in 95.07745269099905
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,908 | app.py:225 | app_fit: losses_distributed []
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,909 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,910 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,912 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 16:41:21,914 | app.py:229 | app_fit: metrics_centralized {}
myFlowerExperiment | INFO flwr 2023-09-29 17:02:24,849 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
myFlowerExperiment | INFO flwr 2023-09-29 17:02:32,237 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'node:__internal_head__': 1.0, 'memory': 4081392846.0, 'object_store_memory': 2040696422.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2023-09-29 17:02:32,238 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2023-09-29 17:02:32,260 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2023-09-29 17:02:32,262 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2023-09-29 17:02:32,264 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 17:02:44,221 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2023-09-29 17:02:44,229 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2023-09-29 17:02:44,232 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2023-09-29 17:02:44,238 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:02:55,499 | server.py:236 | fit_round 1 received 2 results and 0 failures
myFlowerExperiment | WARNING flwr 2023-09-29 17:02:55,514 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 17:02:55,517 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 17:03:04,561 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 17:03:04,563 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:04,566 | server.py:187 | evaluate_round 1 received 1 results and 1 failures
myFlowerExperiment | WARNING flwr 2023-09-29 17:03:04,568 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:04,569 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:15,003 | server.py:236 | fit_round 2 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:15,007 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 17:03:24,149 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 17:03:24,151 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | ERROR flwr 2023-09-29 17:03:24,305 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 17:03:24,307 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27524, ip=172.30.126.159, actor_id=998151bbc80af6b33583e78901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f30e93603d0>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:24,308 | server.py:187 | evaluate_round 2 received 0 results and 2 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:24,309 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:34,908 | server.py:236 | fit_round 3 received 2 results and 0 failures
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:34,911 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)
myFlowerExperiment | ERROR flwr 2023-09-29 17:03:36,740 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 2ed659642acd212c0b8ad181684d3060cc48092694c4b9aa15041607) where the task (actor ID: 998151bbc80af6b33583e78901000000, name=DefaultActor.__init__, pid=27524, memory used=0.63GB) was running was 7.07GB / 7.44GB (0.950731), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d0a93af9b7afb95dd04e6118ee2f1cbf4084539078fb530309323df0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d0a93af9b7afb95dd04e6118ee2f1cbf4084539078fb530309323df0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
20652	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
27523	0.67	ray::DefaultActor.run
27524	0.63	ray::DefaultActor
27522	0.25	ray::DefaultActor
27504	0.25	ray::DefaultActor
27519	0.25	ray::DefaultActor
27514	0.25	ray::DefaultActor
27506	0.25	ray::DefaultActor
27507	0.25	ray::DefaultActor
27516	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2023-09-29 17:03:36,742 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 2ed659642acd212c0b8ad181684d3060cc48092694c4b9aa15041607) where the task (actor ID: 998151bbc80af6b33583e78901000000, name=DefaultActor.__init__, pid=27524, memory used=0.63GB) was running was 7.07GB / 7.44GB (0.950731), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d0a93af9b7afb95dd04e6118ee2f1cbf4084539078fb530309323df0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d0a93af9b7afb95dd04e6118ee2f1cbf4084539078fb530309323df0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
20652	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...
27523	0.67	ray::DefaultActor.run
27524	0.63	ray::DefaultActor
27522	0.25	ray::DefaultActor
27504	0.25	ray::DefaultActor
27519	0.25	ray::DefaultActor
27514	0.25	ray::DefaultActor
27506	0.25	ray::DefaultActor
27507	0.25	ray::DefaultActor
27516	0.25	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2023-09-29 17:03:44,177 | ray_client_proxy.py:147 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 140, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 402, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 288, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)

myFlowerExperiment | ERROR flwr 2023-09-29 17:03:44,178 | ray_client_proxy.py:148 | [36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate
    return maybe_call_evaluate(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate
  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results
UnboundLocalError: local variable 'outputMetricFile' referenced before assignment

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=27523, ip=172.30.126.159, actor_id=ddfa4171f36278a559b95fbd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2e1d1c2490>)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 60, in run\n    job_results = job_fn()\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 215, in evaluate\n    return maybe_call_evaluate(\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/client.py", line 237, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/client/app.py", line 357, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/tmp/ipykernel_20652/810984218.py", line 39, in evaluate\n  File "/tmp/ipykernel_20652/3708332468.py", line 58, in evaluate_and_save_results\nUnboundLocalError: local variable \'outputMetricFile\' referenced before assignment\n',)
myFlowerExperiment | DEBUG flwr 2023-09-29 17:03:44,179 | server.py:187 | evaluate_round 3 received 0 results and 2 failures
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,181 | server.py:153 | FL finished in 59.94640599100239
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,184 | app.py:225 | app_fit: losses_distributed [(1, 0.4718545973300934)]
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,185 | app.py:226 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,186 | app.py:227 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,187 | app.py:228 | app_fit: losses_centralized []
myFlowerExperiment | INFO flwr 2023-09-29 17:03:44,188 | app.py:229 | app_fit: metrics_centralized {}
