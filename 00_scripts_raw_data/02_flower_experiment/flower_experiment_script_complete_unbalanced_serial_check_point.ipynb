{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0c80d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from multiprocessing import Process\n",
    "import gc\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "#!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from flwr.common.logger import log\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2257ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total arguments passed: 3\n",
      "arg: /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py\n",
      "no\n",
      "arg: -f\n",
      "no\n",
      "arg: /home/guilherme/.local/share/jupyter/runtime/kernel-9a089fe6-fcbb-413a-9cc4-c3fa15f1fada.json\n",
      "no\n",
      "iteracoes: 0\n"
     ]
    }
   ],
   "source": [
    "# argumentos\n",
    "n = len(sys.argv)\n",
    "print(\"Total arguments passed:\", n)\n",
    "iteracoes = 0\n",
    "finalIterations = 0\n",
    "if(n > 0):\n",
    "    for value in sys.argv:\n",
    "        print(\"arg:\", value)\n",
    "        try:\n",
    "            iteracoes = int(value)\n",
    "            break\n",
    "        except:\n",
    "            print(\"no\")\n",
    "print(\"iteracoes:\",0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a4484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder\n",
    "#inputFolders = \"../02-transformed-data-new-testes/dados2019/\"\n",
    "inputFolderPath = \"../data_2019_processed/\"\n",
    "\n",
    "# General configuration\n",
    "NUMBER_OF_ITERATIONS_FINAL = 200\n",
    "    \n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 0\n",
    "\n",
    "# output folder\n",
    "outputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(NUMBER_OF_ITERATIONS_FINAL)\n",
    "#outputFolder = \"test_checkpoint\"\n",
    "checkPointFolder = outputFolder+\"/checkpoints\"\n",
    "\n",
    "# train file name modifier\n",
    "fileSufixTrain = \"\" # _smote for smote\n",
    "\n",
    "fl.common.logger.configure(identifier=\"myFlowerExperiment\", filename=\"log_\"+outputFolder+\".txt\")\n",
    "\n",
    "# usado para checkpoints\n",
    "if(iteracoes > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = iteracoes\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0929b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the folder exists or not\n",
      "The directory exists!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the folder exists or not\")\n",
    "isExist = os.path.exists(outputFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(outputFolder)\n",
    "    print(\"The new directory is created!\")\n",
    "else:\n",
    "    print(\"The directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5249b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the checkpoint folder exists or not\n",
      "The checkpoint directory exists!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the checkpoint folder exists or not\")\n",
    "isExist = os.path.exists(checkPointFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(checkPointFolder)\n",
    "    print(\"The new checkpoint directory is created!\")\n",
    "else:\n",
    "    print(\"The checkpoint directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75b3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features\n",
    "inputFeatures = [\"activity\",\"location\",\"day_of_week\",\"light\",\"phone_lock\",\"proximity\",\"sound\",\"time_to_next_alarm\", \"minutes_day\"]\n",
    "outputClasses = [\"awake\",\"asleep\"]\n",
    "#outputClasses = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7fa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client datasets used on the training process (75% of data)\n",
    "trainFolders =  ['0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                '0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                '2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                '2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                '7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                #'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                #'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4']\n",
    "                #'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                #'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                #'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                #'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                #'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                #'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']\n",
    "            \n",
    "# client datasets used on the training process (25% of data)\n",
    "testFolders =  [#'0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                #'0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                #'2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                #'2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                #'7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                #'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                #'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                #'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                #'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                #'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                #'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                #'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                #'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                #'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                #'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                #'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                #'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                #'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                #'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4'] \n",
    "                'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMetrics(y_test,yhat_probs):\n",
    "    # predict crisp classes for test set deprecated\n",
    "    #yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "    #yhat_classes = np.argmax(yhat_probs,axis=1)\n",
    "    yhat_classes = yhat_probs.round()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    #print(matrix)\n",
    "    \n",
    "    array = []\n",
    "    results = dict()\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1_score'] = f1\n",
    "    results['cohen_kappa_score'] = kappa\n",
    "    results['roc_auc_score'] = auc\n",
    "    results['matrix'] = (\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    results['TP'] = matrix[0][0]\n",
    "    results['FP'] = matrix[0][1]\n",
    "    results['FN'] = matrix[1][0]\n",
    "    results['TN'] = matrix[1][1]\n",
    "    \n",
    "    array.append(accuracy)\n",
    "    array.append(precision)\n",
    "    array.append(recall)\n",
    "    array.append(f1)\n",
    "    array.append(kappa)\n",
    "    array.append(auc)\n",
    "    array.append(\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    array.append(matrix[0][0]) # TP\n",
    "    array.append(matrix[0][1]) # FP\n",
    "    array.append(matrix[1][0]) # FN\n",
    "    array.append(matrix[1][1]) # TN\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "# y_test     = Array with real values\n",
    "# yhat_probs = Array with predicted values\n",
    "def printMetrics(y_test,yhat_probs):\n",
    "    # generate metrics\n",
    "    results, array= generateMetrics(y_test,yhat_probs)\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = results['accuracy']\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = results['precision']\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = results['recall'] \n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = results['f1_score']\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = results['cohen_kappa_score']\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = results['roc_auc_score']\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    print(\"\\Confusion Matrix\")\n",
    "    matrix = results['matrix']\n",
    "    print(matrix)\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "def generateGlobalMetrics(metrics):\n",
    "    accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score = 0,0,0,0,0,0\n",
    "    for metric in metrics:\n",
    "        accuracy = accuracy + metric['accuracy']\n",
    "        precision = precision + metric['precision']\n",
    "        recall = recall + metric['recall']\n",
    "        f1_score = f1_score + metric['f1_score']\n",
    "        cohen_kappa_score = cohen_kappa_score + metric['cohen_kappa_score']\n",
    "        roc_auc_score = roc_auc_score + metric['roc_auc_score']\n",
    "        \n",
    "    # mean\n",
    "    size = len(metrics)\n",
    "    print(size)\n",
    "    accuracy = accuracy / size\n",
    "    precision = precision / size\n",
    "    recall = recall / size\n",
    "    f1_score = f1_score / size\n",
    "    cohen_kappa_score = cohen_kappa_score / size\n",
    "    roc_auc_score = roc_auc_score / size\n",
    "    \n",
    "    return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score]\n",
    "\n",
    "def showGlobalMetrics(metrics):\n",
    "    res = generateGlobalMetrics(metrics)\n",
    "    \n",
    "    accuracy = res[0]\n",
    "    precision = res[1]\n",
    "    recall = res[2]\n",
    "    f1_score = res[3]\n",
    "    cohen_kappa_score = res[4]\n",
    "    roc_auc_score = res[5]\n",
    "    \n",
    "    #show:\\\n",
    "    print(\"accuracy: \",accuracy)\n",
    "    print(\"precision: \",precision)\n",
    "    print(\"recall: \",recall)\n",
    "    print(\"f1_score: \",f1_score)\n",
    "    print(\"cohen_kappa_score: \",cohen_kappa_score)\n",
    "    print(\"roc_auc_score: \",roc_auc_score)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0789f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the list of directories and concat them\n",
    "def loadDataFromFolders(foldersToLoad,inputFolders,fileType = \"\"):\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        if(i == 0):\n",
    "            temp_data = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        else:\n",
    "            dataset = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "            temp_data = pd.concat([temp_data, dataset])\n",
    "    # return the dataset        \n",
    "    return temp_data\n",
    "\n",
    "# take the list of directories and concat them\n",
    "def loadDataFromFoldersOnList(foldersToLoad,inputFolders,fileType = \"\"):\n",
    "    clientList = []\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        temp_data = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        print(\"Adding to the list: \", temp_data.shape)\n",
    "        clientList.append(temp_data)\n",
    "    # return the dataset        \n",
    "    return clientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0170ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data\n",
      "8 datasets\n",
      "0 - rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw ../data_2019_processed/student_rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw_transformed.csv\n",
      "1 - RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI ../data_2019_processed/student_RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI_transformed.csv\n",
      "2 - VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is ../data_2019_processed/student_VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is_transformed.csv\n",
      "3 - Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw ../data_2019_processed/student_Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw_transformed.csv\n",
      "4 - XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA ../data_2019_processed/student_XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA_transformed.csv\n",
      "5 - YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw ../data_2019_processed/student_YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw_transformed.csv\n",
      "6 - ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM ../data_2019_processed/student_ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM_transformed.csv\n",
      "7 - ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY ../data_2019_processed/student_ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY_transformed.csv\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float64\n",
      " 1   location            134888 non-null  float64\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float64\n",
      " 4   sound               134888 non-null  float64\n",
      " 5   proximity           134888 non-null  float64\n",
      " 6   phone_lock          134888 non-null  float64\n",
      " 7   light               134888 non-null  float64\n",
      " 8   day_of_week         134888 non-null  float64\n",
      " 9   minutes_day         134888 non-null  float64\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      "dtypes: float64(10), object(2)\n",
      "memory usage: 13.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_to_next_alarm</th>\n",
       "      <th>sound</th>\n",
       "      <th>proximity</th>\n",
       "      <th>phone_lock</th>\n",
       "      <th>light</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>minutes_day</th>\n",
       "      <th>timestamp_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678249</td>\n",
       "      <td>2018-05-14 16:16:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.211282e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678944</td>\n",
       "      <td>2018-05-14 16:17:39+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679639</td>\n",
       "      <td>2018-05-14 16:18:39+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680334</td>\n",
       "      <td>2018-05-14 16:19:09+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681028</td>\n",
       "      <td>2018-05-14 16:20:09+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23747</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819100e-03</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.510076</td>\n",
       "      <td>2018-06-13 12:14:37+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23748</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819743e-03</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.512856</td>\n",
       "      <td>2018-06-13 12:18:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23749</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819743e-03</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>2018-06-13 12:19:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23750</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.820064e-03</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>2018-06-13 12:19:38+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.820064e-03</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.514246</td>\n",
       "      <td>2018-06-13 12:20:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134888 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity  location     timestamp  time_to_next_alarm     sound  \\\n",
       "0          0.75       1.0  0.000000e+00            0.000000  0.515992   \n",
       "1          0.25       1.0  3.211282e-07            0.000000  0.542171   \n",
       "2          0.25       1.0  6.422564e-07            0.000000  0.515992   \n",
       "3          0.00       1.0  6.422564e-07            0.000000  0.515992   \n",
       "4          0.25       1.0  6.422564e-07            0.000000  0.531341   \n",
       "...         ...       ...           ...                 ...       ...   \n",
       "23747      0.25       1.0  5.819100e-03            0.000099  0.000000   \n",
       "23748      0.25       1.0  5.819743e-03            0.000694  0.000000   \n",
       "23749      0.25       1.0  5.819743e-03            0.000595  0.000000   \n",
       "23750      0.25       1.0  5.820064e-03            0.000595  0.000000   \n",
       "23751      0.50       1.0  5.820064e-03            0.000496  0.000000   \n",
       "\n",
       "       proximity  phone_lock     light  day_of_week  minutes_day  \\\n",
       "0            1.0         0.0  0.000000     1.000000     0.678249   \n",
       "1            0.0         1.0  0.000007     1.000000     0.678944   \n",
       "2            0.0         1.0  0.000000     1.000000     0.679639   \n",
       "3            0.0         1.0  0.000000     1.000000     0.680334   \n",
       "4            0.0         1.0  0.000000     1.000000     0.681028   \n",
       "...          ...         ...       ...          ...          ...   \n",
       "23747        1.0         1.0  0.000236     0.166667     0.510076   \n",
       "23748        1.0         1.0  0.000325     0.166667     0.512856   \n",
       "23749        1.0         1.0  0.000325     0.166667     0.513551   \n",
       "23750        1.0         1.0  0.000354     0.166667     0.513551   \n",
       "23751        0.0         1.0  0.000000     0.166667     0.514246   \n",
       "\n",
       "                  timestamp_text  class  \n",
       "0      2018-05-14 16:16:08+00:00  awake  \n",
       "1      2018-05-14 16:17:39+00:00  awake  \n",
       "2      2018-05-14 16:18:39+00:00  awake  \n",
       "3      2018-05-14 16:19:09+00:00  awake  \n",
       "4      2018-05-14 16:20:09+00:00  awake  \n",
       "...                          ...    ...  \n",
       "23747  2018-06-13 12:14:37+00:00  awake  \n",
       "23748  2018-06-13 12:18:08+00:00  awake  \n",
       "23749  2018-06-13 12:19:08+00:00  awake  \n",
       "23750  2018-06-13 12:19:38+00:00  awake  \n",
       "23751  2018-06-13 12:20:08+00:00  awake  \n",
       "\n",
       "[134888 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Preparing test data\")\n",
    " \n",
    "# test data comprising 25% of the data. It must be fixed to all models being evaluated\n",
    "#X_test  = pd.read_csv(inputFolders+\"test/allData-classification-numeric-normalized.csv\")\n",
    "X_test = loadDataFromFolders(testFolders,inputFolderPath,\"\")\n",
    "\n",
    "print()\n",
    "# undestand the dataset by looking on their infos\n",
    "print(X_test.info())\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bed070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing X_train data\n",
      "2 datasets\n",
      "0 - 0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs ../data_2019_processed/student_0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs_transformed.csv\n",
      "Adding to the list:  (17993, 12)\n",
      "1 - SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4 ../data_2019_processed/student_SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4_transformed.csv\n",
      "Adding to the list:  (12709, 12)\n",
      "Total 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing X_train data\")\n",
    "# load cliend data\n",
    "clientList = loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)\n",
    "        \n",
    "NUMBER_OF_CLIENTS = len(clientList)\n",
    "print(\"Total\",(len(clientList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c86ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float64\n",
      " 1   location            134888 non-null  float64\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float64\n",
      " 4   sound               134888 non-null  float64\n",
      " 5   proximity           134888 non-null  float64\n",
      " 6   phone_lock          134888 non-null  float64\n",
      " 7   light               134888 non-null  float64\n",
      " 8   day_of_week         134888 non-null  float64\n",
      " 9   minutes_day         134888 non-null  float64\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      " 12  awake               134888 non-null  uint8  \n",
      " 13  asleep              134888 non-null  uint8  \n",
      "dtypes: float64(10), object(2), uint8(2)\n",
      "memory usage: 13.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding function\n",
    "def transform_output_nominal_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data['awake']\n",
    "    dataset['asleep'] = one_hot_encoded_data['asleep']\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# one-hot encoding function\n",
    "def transform_output_numerical_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data[0]\n",
    "    dataset['asleep'] = one_hot_encoded_data[1]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# transform output to one_hot_encoding for the testing dataset\n",
    "X_test = transform_output_nominal_class_into_one_hot_encoding(X_test)\n",
    "\n",
    "# transform output to one_hot_encoding for the input dataset\n",
    "for i in range(0,len(clientList)):\n",
    "    clientList[i] = transform_output_nominal_class_into_one_hot_encoding(clientList[i])\n",
    "    #print (clientList[i])\n",
    "    \n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39a58ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float32\n",
      " 1   location            134888 non-null  float32\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float32\n",
      " 4   sound               134888 non-null  float32\n",
      " 5   proximity           134888 non-null  float32\n",
      " 6   phone_lock          134888 non-null  float32\n",
      " 7   light               134888 non-null  float32\n",
      " 8   day_of_week         134888 non-null  float32\n",
      " 9   minutes_day         134888 non-null  float32\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      " 12  awake               134888 non-null  float32\n",
      " 13  asleep              134888 non-null  float32\n",
      "dtypes: float32(11), float64(1), object(2)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "def transform_data_type(dataframe):\n",
    "    \n",
    "    # transform inputs\n",
    "    for column in inputFeatures:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    # transform outputs\n",
    "    for column in outputClasses:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# transforms the data\n",
    "X_test = transform_data_type(X_test)\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec93564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepering the test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepering the test dataset\")\n",
    "# selects the data to train and test\n",
    "X_test_data = X_test[inputFeatures]\n",
    "y_test_label = X_test[outputClasses]\n",
    "\n",
    "# transtorm data to tensor slices\n",
    "#client_test_dataset = tf.data.Dataset.from_tensor_slices((X_test_data.values, y_test_label.values))\n",
    "\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "\n",
    "#print(client_test_dataset.element_spec)\n",
    "#client_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da70f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"preparing the training datasets\")\n",
    "#federated_training_data = []\n",
    "# transform the data\n",
    "#for i in range(0,len(clientList)):\n",
    "#    # selects the data to train and test\n",
    "#    data   = clientList[i][inputFeatures]\n",
    "#    labels = clientList[i][outputClasses]\n",
    "#    # transform the data to tensor slices\n",
    "#    client_train_dataset = tf.data.Dataset.from_tensor_slices((data.values, labels.values))\n",
    "    # apply the configs\n",
    "#    client_train_dataset = client_train_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "    # transform the data to\n",
    " #   federated_training_data.append(client_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a9b73de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                160       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314\n",
      "Trainable params: 314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 00:34:14.765709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:14.796742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:14.796991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:14.797868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 00:34:14.799581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:14.799816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:14.800072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:15.785066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:15.785841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:15.785860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-10 00:34:15.786133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 00:34:15.786463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3421 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model\")\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(9,)),\n",
    "      #tf.keras.layers.Dense(9, activation=tf.keras.activations.relu), \n",
    "      tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(8, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
    "      #tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "keras_model = create_keras_model()\n",
    "#keras_model.summary()\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35447b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data (MobileNetV2, CIFAR-10)\n",
    "#model = keras_model\n",
    "#model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362b0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(keras_model,X_test_data, y_test_label, current_round_index, \n",
    "                              clientId, prefix_string = \"Results\", lossValue = -1):\n",
    "     # predict values\n",
    "    yhat_probs = keras_model.predict(X_test_data,verbose=VERBOSE)\n",
    "    \n",
    "    # as we deal with a classification problem with one hot encoding, we must round the values to 0 and 1.\n",
    "    yhat_probs_rounded = yhat_probs.round()\n",
    "    \n",
    "    # create a dataframe with the predicted data\n",
    "    y_predicted_df = pd.DataFrame(data=yhat_probs_rounded,columns=['awake','asleep']) \n",
    "    #y_test_label_label = pd.DataFrame(data=y_test_label,columns=['awake','asleep']) \n",
    "    \n",
    "    roundData = []\n",
    "\n",
    "    columns = ['client','round','loss','class','accuracy','precision','recall', \n",
    "               'f1_score','cohen_kappa_score','roc_auc_score','confusion_matrix',\n",
    "               'TP','FP','FN','TN']\n",
    "    \n",
    "    # Instantiate the list that will contain the results\n",
    "    listOfMetrics = list()\n",
    "    \n",
    "    #print('awake')    \n",
    "    #res,resA = printMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    res,resA = generateMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    listOfMetrics.append(res)\n",
    "    \n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'awake'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('')\n",
    "    #print('asleep')\n",
    "    #res,resA = printMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    res,resA = generateMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    listOfMetrics.append(res)\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'asleep'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('Global')\n",
    "    #resA = showGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    resA = generateGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'avg'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    dataMetrics = pd.DataFrame(data=roundData,columns=columns) \n",
    "    # write file\n",
    "    if(clientId >= 0):\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\"_MLP_client_\" + str(clientId) + \"_round_\" + str(current_round_index) + \".csv\"\n",
    "    else:\n",
    "        outputMetricFile = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "        # print global model results\n",
    "        if(os.path.isfile(outputMetricFile)):\n",
    "            dataset = pd.read_csv(outputMetricFile)\n",
    "            dataMetrics = pd.concat([dataset, dataMetrics], axis=0)\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "    dataMetrics.to_csv(outputMetricFile, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab1e2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint model test_checkpoint/checkpoints/round-*\n",
      "Loading pre-trained model from:  test_checkpoint/checkpoints/round-2-weights.h5\n",
      "Last round:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint model\",checkPointFolder+\"/round-*\")\n",
    "list_of_files = [fname for fname in glob.glob(checkPointFolder+\"/round-*\")]\n",
    "last_round_checkpoint = -1\n",
    "latest_round_file = None\n",
    "model_check_point = None\n",
    "filename_np = None\n",
    "filename_h5 = None\n",
    "\n",
    "if len(list_of_files) > 0:\n",
    "    latest_round_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(\"Loading pre-trained model from: \", latest_round_file)\n",
    "    if(len(latest_round_file) > 0):\n",
    "        # load the name\n",
    "        last_round = latest_round_file.replace(checkPointFolder+\"/round-\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.npz\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.h5\",\"\")\n",
    "        print(\"Last round: \",last_round)\n",
    "    \n",
    "        last_round_checkpoint = int(last_round)\n",
    "        filename_h5 = checkPointFolder+\"/round-\"+last_round+\"-weights.h5\"\n",
    "        filename_np = checkPointFolder+\"/round-\"+last_round+\"-weights.npz\"\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "\n",
    "    #check_point_model = tf.keras.models.load_model(latest_round_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f8aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "722dabf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_round_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf9bb93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if latest_round_file is not None:\n",
    "#    keras_model.load_weights(latest_round_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5161a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c16138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10217727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteractions after the checkpoint:  2\n"
     ]
    }
   ],
   "source": [
    "if(last_round_checkpoint > -1):\n",
    "    NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL - (last_round_checkpoint)\n",
    "\n",
    "    print(\"Number of iteractions after the checkpoint: \",NUMBER_OF_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        #print(\"TEsteeee\", aggregated_parameters)\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Save aggregated_ndarrays\n",
    "            print(f\"Saving round {server_round} aggregated_ndarrays...\")\n",
    "            fileName = f\"{checkPointFolder}/round-{server_round}-weights.npz\"\n",
    "            print(fileName)\n",
    "            #print(aggregated_parameters)\n",
    "            print()\n",
    "            np.savez(fileName, *aggregated_ndarrays)\n",
    "            #np.savez(fileName+\"2\", *aggregated_parameters)\n",
    "            #keras_model = create_keras_model()\n",
    "            #keras_model.set_weights(aggregated_parameters)\n",
    "            #keras_model.save_weights(fileName)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c71c6242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declarating client function\n"
     ]
    }
   ],
   "source": [
    "print(\"Declarating client function\")\n",
    "\n",
    "# Define a Flower client\n",
    "class FlowerISABELASleepClient(fl.client.NumPyClient):\n",
    "\n",
    "    def __init__(self, clientId, model, X_train_data, y_train_label,round_index=0):\n",
    "        self.round_index = round_index\n",
    "        self.clientId = clientId\n",
    "        self.model = model\n",
    "        self.X_train_data = X_train_data\n",
    "        self.y_train_label = y_train_label\n",
    "\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current weights.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Fit model and return new weights as well as number of training examples.\"\"\"\n",
    "        self.model.set_weights(parameters)\n",
    "        \n",
    "        # use the checkpoint if it is not -1\n",
    "        #if(last_round_checkpoint == self.round_index and last_round_checkpoint != -1):\n",
    "        #    print(\"loading checkpoint: \",filename_h5, \" to client \",self.clientId)\n",
    "        #    print(\"loading\", latest_round_file)\n",
    "        #    self.model = tf.keras.models.load_weights(filename_h5)\n",
    "            \n",
    "        self.model.fit(self.X_train_data, self.y_train_label, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)\n",
    "\n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = self.model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # print model results\n",
    "        evaluate_and_save_results(self.model,X_test_data, y_test_label, self.round_index, self.clientId,\"global\",loss)\n",
    "        return self.model.get_weights(), len(self.X_train_data), {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c336f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn( model):\n",
    "    \n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \n",
    "        current_round = server_round\n",
    "        \n",
    "        if(last_round_checkpoint > -1):\n",
    "            current_round = server_round + last_round_checkpoint\n",
    "            \n",
    "        print(\"Evaluating global model round\",current_round)\n",
    "        \n",
    "        model.set_weights(parameters)\n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # only saves if the server_round + last_round_checkpoint != last_round_checkpoint to avaid duble metrics\n",
    "        if(current_round > last_round_checkpoint):\n",
    "            # print model results\n",
    "            evaluate_and_save_results(model,X_test_data, y_test_label, current_round, -1,\"global_model_MLP_metrics\",loss)\n",
    "\n",
    "            #checkpoint\n",
    "            fileName = f\"{checkPointFolder}/round-{current_round}-weights.h5\"\n",
    "            model.save_weights(fileName)\n",
    "        else:\n",
    "            print(\"Round already evaluated\")\n",
    "        \n",
    "        return loss, { 'accuracy': accuracy }\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e8c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-10-10 00:34:26,282 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
      "2023-10-10 00:34:28,598\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2023-10-10 00:34:29,754 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:172.30.126.159': 1.0, 'CPU': 16.0, 'memory': 3666739200.0, 'object_store_memory': 1833369600.0}\n",
      "INFO flwr 2023-10-10 00:34:29,755 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}\n",
      "INFO flwr 2023-10-10 00:34:29,778 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors\n",
      "INFO flwr 2023-10-10 00:34:29,781 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2023-10-10 00:34:29,783 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-10-10 00:34:29,785 | server.py:91 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating global model round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 00:34:32.995227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-10-10 00:35:00,473 | server.py:94 | initial parameters (loss, other metrics): 0.5064367055892944, {'accuracy': 0.7957342267036438}\n",
      "INFO flwr 2023-10-10 00:35:00,474 | server.py:104 | FL starting\n",
      "DEBUG flwr 2023-10-10 00:35:00,475 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client: 1 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 1\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data X: 12709\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data Y: 12709\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 1 round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2023-10-10 00:35:00,926 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2023-10-10 00:35:00,927 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2023-10-10 00:35:09,274 | server.py:236 | fit_round 1 received 1 results and 1 failures\n",
      "WARNING flwr 2023-10-10 00:35:09,277 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 1 aggregated_ndarrays...\n",
      "test_checkpoint/checkpoints/round-1-weights.npz\n",
      "\n",
      "Evaluating global model round 3\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m Data X: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m Data Y: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3193)\u001b[0m Creating client model to client: 0 round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-10-10 00:35:28,512 E 2744 2744] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c, IP: 172.30.126.159) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.30.126.159`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "INFO flwr 2023-10-10 00:35:33,783 | server.py:125 | fit progress: (1, 0.49569064378738403, {'accuracy': 0.8144312500953674}, 33.30851527399864)\n",
      "DEBUG flwr 2023-10-10 00:35:33,784 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2023-10-10 00:35:33,820 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2023-10-10 00:35:33,821 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2023-10-10 00:35:33,842 | server.py:187 | evaluate_round 1 received 0 results and 2 failures\n",
      "DEBUG flwr 2023-10-10 00:35:33,843 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
      "ERROR flwr 2023-10-10 00:35:33,877 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2023-10-10 00:35:33,878 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data X: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data Y: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 0 round 4\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client: 1 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 1\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data X: 12709\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data Y: 12709\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 1 round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-10-10 00:35:41,384 | server.py:236 | fit_round 2 received 1 results and 1 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m 2\n",
      "Saving round 2 aggregated_ndarrays...\n",
      "test_checkpoint/checkpoints/round-2-weights.npz\n",
      "\n",
      "Evaluating global model round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-10-10 00:36:04,671 | server.py:125 | fit progress: (2, 0.49953171610832214, {'accuracy': 0.8134526610374451}, 64.19612357000005)\n",
      "DEBUG flwr 2023-10-10 00:36:04,672 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
      "ERROR flwr 2023-10-10 00:36:04,702 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2023-10-10 00:36:04,703 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: c9ca792ebbaf5c462380bcb328105860bafe5489ac76a7fe4ec2d27c) where the task (actor ID: 3bc0030d3795b19456626f5901000000, name=DefaultActor.__init__, pid=3193, memory used=0.28GB) was running was 7.07GB / 7.44GB (0.95118), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-d8c12bfd4137e7382f723490b71486099c923e80726d322ae3a8a430*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "2530\t2.21\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/tf/bin/python -m ipykernel_launcher -f /home/...\n",
      "3193\t0.28\tray::DefaultActor.run\n",
      "3192\t0.27\tray::DefaultActor.run\n",
      "3188\t0.25\tray::DefaultActor\n",
      "3177\t0.25\tray::DefaultActor\n",
      "3184\t0.25\tray::DefaultActor\n",
      "3180\t0.25\tray::DefaultActor\n",
      "3183\t0.25\tray::DefaultActor\n",
      "3186\t0.25\tray::DefaultActor\n",
      "3189\t0.25\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-10-10 00:36:04,725 | server.py:187 | evaluate_round 2 received 0 results and 2 failures\n",
      "INFO flwr 2023-10-10 00:36:04,726 | server.py:153 | FL finished in 64.2516323819982\n",
      "INFO flwr 2023-10-10 00:36:04,727 | app.py:225 | app_fit: losses_distributed []\n",
      "INFO flwr 2023-10-10 00:36:04,728 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2023-10-10 00:36:04,729 | app.py:227 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-10-10 00:36:04,730 | app.py:228 | app_fit: losses_centralized [(0, 0.5064367055892944), (1, 0.49569064378738403), (2, 0.49953171610832214)]\n",
      "INFO flwr 2023-10-10 00:36:04,731 | app.py:229 | app_fit: metrics_centralized {'accuracy': [(0, 0.7957342267036438), (1, 0.8144312500953674), (2, 0.8134526610374451)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, centralized):\n",
       "\tround 0: 0.5064367055892944\n",
       "\tround 1: 0.49569064378738403\n",
       "\tround 2: 0.49953171610832214\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, 0.7957342267036438), (1, 0.8144312500953674), (2, 0.8134526610374451)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data X: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Data Y: 17993\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=3192)\u001b[0m Creating client model to client: 0 round 5\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import math\n",
    "# Create an instance of the model and get the parameters\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "#if DEVICE.type == \"cuda\":\n",
    "\n",
    "client_resources = {\"num_cpus\": 1}\n",
    "\n",
    "#keras_model = create_keras_model()\n",
    "keras_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerISABELASleepClient:\n",
    "    print(\"starting client: \"+str(cid),type(cid))\n",
    "    #convert client ID to int\n",
    "    clientId = int(cid)\n",
    "    print(\"starting client: \", type(clientId))\n",
    "\n",
    "    data   = clientList[clientId][inputFeatures]\n",
    "    labels = clientList[clientId][outputClasses]\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid))\n",
    "    print(\"Data X: \"+str(len(data)))\n",
    "    print(\"Data Y: \"+str(len(labels)))\n",
    "    \n",
    "    file_global_model = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "    index_round = 0 \n",
    "    \n",
    "    # get last\n",
    "    if(os.path.isfile(file_global_model)):\n",
    "        dataset = pd.read_csv(file_global_model)\n",
    "        index_round = dataset[\"round\"].max() + 1\n",
    "        del dataset\n",
    "    \n",
    "    # update the index round in the previous checkpoint\n",
    "    if(last_round_checkpoint > -1 and (index_round == last_round_checkpoint)):\n",
    "        index_round = last_round_checkpoint\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid),\"round\",index_round)\n",
    "    # Load and compile a Keras model for CIFAR-10\n",
    "    model = create_keras_model()\n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return FlowerISABELASleepClient(clientId,model,data,labels,index_round)\n",
    "\n",
    "strategy = SaveModelStrategy(\n",
    "    min_available_clients=NUMBER_OF_CLIENTS,\n",
    "    evaluate_fn=get_evaluate_fn(keras_model)\n",
    ") # (same arguments as FedAvg here)\n",
    "\n",
    "# load checkpoint\n",
    "if(filename_h5 is not None):\n",
    "    \n",
    "    #npzFile = np.load(filename_np)\n",
    "    keras_model.load_weights(filename_h5)\n",
    "    \n",
    "    initial_parameters = keras_model.get_weights() \n",
    "    # Convert the weights (np.ndarray) to parameters (bytes)\n",
    "    init_param = fl.common.ndarrays_to_parameters(initial_parameters)\n",
    "\n",
    "    strategy = SaveModelStrategy(\n",
    "        min_available_clients=NUMBER_OF_CLIENTS,\n",
    "        evaluate_fn=get_evaluate_fn(keras_model),\n",
    "        initial_parameters = init_param\n",
    "    ) # (same arguments as FedAvg here)\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUMBER_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUMBER_OF_ITERATIONS),  # Just three rounds\n",
    "    client_resources=client_resources,\n",
    "    strategy = strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f400a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2c363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
